{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten, Dropout\n",
    "import keras.optimizers\n",
    "from keras.models import load_model\n",
    "import keras.regularizers\n",
    "from keras.regularizers import l2, l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_hash(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"/data/jrgillick/laughter/stored_features1/train_set/\"\n",
    "VAL_DIR = \"/data/jrgillick/laughter/stored_features1/val_set/\"\n",
    "TEST_DIR = \"/data/jrgillick/laughter/stored_features1/test_set/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_laughter_and_speech_clips(directory):\n",
    "    laughter_files = [directory + f for f in os.listdir(directory) if 'laughter' in f]\n",
    "    speech_files = [directory + f for f in os.listdir(directory) if not 'laughter' in f]\n",
    "\n",
    "    laughter_data = [load_hash(f) for f in laughter_files]\n",
    "\n",
    "    laughter_clips = []\n",
    "    for f in laughter_data:\n",
    "        for clip in f:\n",
    "            laughter_clips.append(clip)\n",
    "        \n",
    "    speech_clips = [load_hash(f) for f in speech_files]\n",
    "    \n",
    "    return (laughter_clips, speech_clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_laughter_inputs(clip):\n",
    "    mfcc_feat = clip['mfcc']\n",
    "    delta_feat = clip['delta']\n",
    "    labels = clip['labels']\n",
    "    laughter_frame_indices = np.nonzero(labels)[0]\n",
    "    X = None\n",
    "    for index in laughter_frame_indices:\n",
    "        #print index\n",
    "        features = np.append(mfcc_feat[index-window_size:index+window_size],delta_feat[index-window_size:index+window_size])\n",
    "        if X is None:\n",
    "            X = features\n",
    "        else:\n",
    "            X = np.vstack([X,features])\n",
    "    return (X,np.ones(len(laughter_frame_indices)))\n",
    "\n",
    "def format_speech_inputs(clip):\n",
    "    mfcc_feat = clip['mfcc']\n",
    "    delta_feat = clip['delta']\n",
    "    labels = clip['labels']\n",
    "    speech_frame_indices = np.array(list(xrange(len(labels))))[window_size:-window_size]\n",
    "    X = []\n",
    "    for index in speech_frame_indices:\n",
    "        #print index\n",
    "        features = np.append(mfcc_feat[index-window_size:index+window_size],delta_feat[index-window_size:index+window_size])\n",
    "        X.append(features)\n",
    "    return (np.array(X),np.zeros(len(speech_frame_indices)))\n",
    "\n",
    "def format_laughter_clips(laughter_clips):\n",
    "    formatted_laughter_clips = []\n",
    "    for index, clip in enumerate(laughter_clips):\n",
    "        if index % 500 == 0: print \"formatting %d out of %d\" % (index, len(laughter_clips))\n",
    "        formatted_laughter_clips.append(format_laughter_inputs(clip))\n",
    "    return formatted_laughter_clips\n",
    "    \n",
    "def format_speech_clips(speech_clips):\n",
    "    formatted_speech_clips = []\n",
    "    for index, clip in enumerate(speech_clips):\n",
    "        if index % 500 == 0: print \"formatting %d out of %d\" % (index, len(speech_clips))\n",
    "        formatted_speech_clips.append(format_speech_inputs(clip))\n",
    "    return formatted_speech_clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_data_and_labels(formatted_laughter_clips, formatted_speech_clips):\n",
    "    train_data = []; train_labels = []\n",
    "    for j in xrange(len(formatted_laughter_clips)):\n",
    "        #print \"Processing %d of %d\" % (j,len(formatted_laughter_clips))\n",
    "        clip, label = formatted_laughter_clips[j]\n",
    "        if not clip is None and not label is None:\n",
    "            for i in xrange(len(clip)):\n",
    "                train_data.append(clip[i])\n",
    "                train_labels.append(label[i])\n",
    "\n",
    "    for j in xrange(len(formatted_speech_clips)):\n",
    "        #print \"Processing %d of %d\" % (j,len(formatted_speech_clips))\n",
    "        clip, label = formatted_speech_clips[j]\n",
    "        if not clip is None and not label is None:\n",
    "            for i in xrange(len(clip)):\n",
    "                train_data.append(clip[i])\n",
    "                train_labels.append(label[i])\n",
    "                \n",
    "    return (train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_and_labels_from_dir(directory):\n",
    "    laughter_clips, speech_clips = get_laughter_and_speech_clips(directory)\n",
    "    formatted_laughter_clips = format_laughter_clips(laughter_clips)\n",
    "    formatted_speech_clips = format_speech_clips(speech_clips)\n",
    "    train_data, train_labels = format_data_and_labels(formatted_laughter_clips, formatted_speech_clips)\n",
    "    return (train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_data_and_labels_into_parts(train_data,train_labels,part_size=5):\n",
    "    train_data_parts = []\n",
    "    train_label_parts = []\n",
    "    i = 0\n",
    "    while i < len(train_data) - part_size:\n",
    "        train_data_parts.append(train_data[i:i+part_size])\n",
    "        train_label_parts.append(train_labels[i:i+part_size])\n",
    "        i += part_size\n",
    "    return (train_data_parts, train_label_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_subset(train_data_parts, train_label_parts, start, end):\n",
    "    X = np.vstack(train_data_parts[start:end])\n",
    "    y = np.hstack(train_label_parts[start:end])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "laughter_clips, speech_clips = get_laughter_and_speech_clips(TRAIN_DIR)\n",
    "# Remove some clips that were failing\n",
    "del laughter_clips[677]\n",
    "del laughter_clips[6079]\n",
    "del laughter_clips[7235]\n",
    "formatted_laughter_clips = format_laughter_clips(laughter_clips)\n",
    "formatted_speech_clips = format_speech_clips(speech_clips)\n",
    "train_data, train_labels = format_data_and_labels(formatted_laughter_clips, formatted_speech_clips)\n",
    "train_data_parts, train_label_parts = divide_data_and_labels_into_parts(train_data,train_labels,part_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_laughter_clips, val_speech_clips = get_laughter_and_speech_clips(VAL_DIR)\n",
    "val_formatted_laughter_clips = format_laughter_clips(val_laughter_clips)\n",
    "val_formatted_speech_clips = format_speech_clips(val_speech_clips)\n",
    "val_data, val_labels = format_data_and_labels(val_formatted_laughter_clips, val_formatted_speech_clips)\n",
    "val_data_parts, val_label_parts = divide_data_and_labels_into_parts(val_data,val_labels,part_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, use_bias=True,input_dim=1924))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_on_parts(train_data_parts, train_label_parts):\n",
    "    train_data_parts, train_label_parts = shuffle(train_data_parts, train_label_parts, random_state=0)\n",
    "    i = 0\n",
    "    accs = []\n",
    "    while i < len(train_data_parts):\n",
    "        #print i\n",
    "        X_subset, y_subset = get_data_subset(train_data_parts, train_label_parts, i, i+2000)\n",
    "        model.fit(X_subset,y_subset,shuffle=True,batch_size = 500, epochs=1,verbose=False)\n",
    "        acc = model.evaluate(X_subset, y_subset,verbose=False)[1]\n",
    "        accs.append(acc)\n",
    "        #print np.mean(accs)\n",
    "        i += 2000\n",
    "    print \"Training accuracy %f\" % (np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_on_parts(data_parts, label_parts):\n",
    "    #train_data_parts, train_label_parts = shuffle(train_data_parts, train_label_parts, random_state=0)\n",
    "    i = 0\n",
    "    accs = []\n",
    "    while i < len(data_parts):\n",
    "        #if i % 10000 == 0: print i\n",
    "        X_subset, y_subset = get_data_subset(data_parts, label_parts, i, i+100)\n",
    "        #model.fit(X_subset,y_subset,shuffle=True,batch_size = 2000, epochs=1,verbose=False)\n",
    "        acc = model.evaluate(X_subset, y_subset,verbose=False)[1]\n",
    "        accs.append(acc)\n",
    "        i += 100\n",
    "    print \"Validation accuracy %f \" % (np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training accuracy 0.819953\n",
      "Validation accuracy 0.810259 \n",
      "Epoch 1\n",
      "Training accuracy 0.853212\n",
      "Validation accuracy 0.821743 \n",
      "Epoch 2\n",
      "Training accuracy 0.866210\n",
      "Validation accuracy 0.827857 \n",
      "Epoch 3\n",
      "Training accuracy 0.874704\n",
      "Validation accuracy 0.832835 \n",
      "Epoch 4\n",
      "Training accuracy 0.881175\n",
      "Validation accuracy 0.835605 \n",
      "Epoch 5\n",
      "Training accuracy 0.886035\n",
      "Validation accuracy 0.835831 \n",
      "Epoch 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-aa637d1b60e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Epoch %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_on_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mevaluate_on_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-bb8b39978933>\u001b[0m in \u001b[0;36mtrain_on_parts\u001b[0;34m(train_data_parts, train_label_parts)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mX_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_subset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_subset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_subset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print np.mean(accs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jrgillick/.local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m    883\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jrgillick/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         return self._test_loop(f, ins,\n\u001b[1;32m   1541\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1542\u001b[0;31m                                verbose=verbose)\n\u001b[0m\u001b[1;32m   1543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jrgillick/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jrgillick/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in xrange(25):\n",
    "    print \"Epoch %d\" % (epoch)\n",
    "    train_on_parts(train_data_parts, train_label_parts)\n",
    "    evaluate_on_parts(val_data_parts, val_label_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train and test on a little bit of data for exploration\n",
    "\n",
    "clip = laughter_data[560][0]\n",
    "speech_clip = speech_data[1]\n",
    "clip2 = laughter_data[610][0]\n",
    "speech_clip2 = speech_data[61]\n",
    "X, y = format_laughter_inputs(clip)\n",
    "X1, y1 = format_speech_inputs(speech_clip)\n",
    "X2, y2 = format_laughter_inputs(clip2)\n",
    "X3, y3 = format_speech_inputs(speech_clip2)\n",
    "\n",
    "X_train = np.vstack([X,X1])\n",
    "y_train = np.append(y,y1)\n",
    "\n",
    "X_test = np.vstack([X2,X3])\n",
    "y_test = np.append(y2,y3)\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "model.fit(X_train,y_train,shuffle=True,batch_size = 32, epochs=1)\n",
    "\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
